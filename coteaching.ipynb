{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import tarfile\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as tt\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToTensor\n",
        "%matplotlib inline\n",
        "\n",
        "matplotlib.rcParams['figure.facecolor'] = '#ffffff'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set shape: (9999, 32, 32, 3)\n",
            "Test labels shape: (9999,)\n",
            "Training set shape: (49999, 32, 32, 3)\n",
            "Training labels shape: (49999,)\n"
          ]
        }
      ],
      "source": [
        "import datasets\n",
        "\n",
        "dataset = datasets.C100Dataset('dataset/data/cifar100_nl.csv', 'dataset/data/cifar100_nl_test.csv')\n",
        "[trainData, trainLabels, testData, testLabels] = dataset.getDataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m trainData, valData, trainLabels, valLabels \u001b[39m=\u001b[39m train_test_split(trainData, trainLabels, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Calculate the mean\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(trainData, axis\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))  \u001b[39m# Compute mean along each channel\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m# Calculate the standard deviation\u001b[39;00m\n\u001b[0;32m     12\u001b[0m std \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstd(trainData, axis\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))  \u001b[39m# Compute standard deviation along each channel\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Assuming trainData, trainLabels, valData, and valLabels are numpy arrays\n",
        "trainData, valData, trainLabels, valLabels = train_test_split(trainData, trainLabels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Calculate the mean\n",
        "mean = np.mean(trainData, axis=(0, 1, 2))  # Compute mean along each channel\n",
        "\n",
        "# Calculate the standard deviation\n",
        "std = np.std(trainData, axis=(0, 1, 2))  # Compute standard deviation along each channel\n",
        "\n",
        "print(\"Mean:\", mean)\n",
        "print(\"Standard Deviation:\", std)\n",
        "\n",
        "# Mean: [0.50764684 0.48674372 0.44104357]\n",
        "# Standard Deviation: [0.26745035 0.25658456 0.27634654]\n",
        "stats = ((0.50764684, 0.48674372, 0.44104357), (0.26745035, 0.25658456, 0.27634654))\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),    # Convert to tensor\n",
        "    transforms.Normalize((0.50764684, 0.48674372, 0.44104357), (0.26745035, 0.25658456, 0.27634654))    # Normalize the data\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "trainData = torch.stack([transform(image) for image in trainData])\n",
        "valData = torch.stack([transform(image) for image in valData])\n",
        "testData = torch.stack([transform(image) for image in testData])\n",
        "\n",
        "# Convert numpy arrays to torch tensors\n",
        "trainLabels = torch.from_numpy(trainLabels)\n",
        "valLabels = torch.from_numpy(valLabels)\n",
        "testLabels = torch.from_numpy(testLabels)\n",
        "\n",
        "# Convert to float\n",
        "trainData = trainData.to(torch.float32)\n",
        "trainLabels = trainLabels.to(torch.int64)\n",
        "valData = valData.to(torch.float32)\n",
        "valLabels = valLabels.to(torch.int64)\n",
        "testData = testData.to(torch.float32)\n",
        "testLabels = testLabels.to(torch.int64)\n",
        "\n",
        "# Create TensorDatasets\n",
        "trainDataset = TensorDataset(trainData, trainLabels)\n",
        "valDataset = TensorDataset(valData, valLabels)\n",
        "testDataset = TensorDataset(testData, testLabels)\n",
        "\n",
        "# Create DataLoaders\n",
        "trainLoader = DataLoader(trainDataset, batch_size=128, shuffle=True)\n",
        "valLoader = DataLoader(valDataset, batch_size=128, shuffle=False)\n",
        "testLoader = DataLoader(testDataset, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show image for fun\n",
        "def denormalize(images, means, stds):\n",
        "    means = torch.tensor(means).reshape(1, 3, 1, 1)\n",
        "    stds = torch.tensor(stds).reshape(1, 3, 1, 1)\n",
        "    return images * stds + means\n",
        "\n",
        "def show_batch(dl):\n",
        "    for images, labels in dl:\n",
        "        fig, ax = plt.subplots(figsize=(12, 12))\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        denorm_images = denormalize(images, *stats)\n",
        "        ax.imshow(make_grid(denorm_images[:64], nrow=8).permute(1, 2, 0).clamp(0,1))\n",
        "        break\n",
        "\n",
        "show_batch(trainLoader)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Architecture and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from itertools import chain\n",
        "from model import CNN\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from absl import logging\n",
        "from utils import EarlyStopping\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def co_teaching_loss(model1_loss, model2_loss, rt):\n",
        "    _, model1_sm_idx = torch.topk(model1_loss, k=int(int(model1_loss.size(0)) * rt), largest=False)\n",
        "    _, model2_sm_idx = torch.topk(model2_loss, k=int(int(model2_loss.size(0)) * rt), largest=False)\n",
        "\n",
        "    # Co-teaching\n",
        "    model1_loss_filter = torch.zeros((model1_loss.size(0))).cuda()\n",
        "    model1_loss_filter[model2_sm_idx] = 1.0\n",
        "    model1_loss = (model1_loss_filter * model1_loss).sum()\n",
        "\n",
        "    model2_loss_filter = torch.zeros((model2_loss.size(0))).cuda()\n",
        "    model2_loss_filter[model1_sm_idx] = 1.0\n",
        "    model2_loss = (model2_loss_filter * model2_loss).sum()\n",
        "\n",
        "    return model1_loss, model2_loss\n",
        "\n",
        "\n",
        "def train_step(data_loader, gpu: bool, model_list: list, optimizer, criterion, rt, warmups):\n",
        "    global_step = 0\n",
        "    avg_accuracy = 0.\n",
        "    avg_loss = 0.\n",
        "\n",
        "    model1, model2 = model_list\n",
        "    model1 = model1.train()\n",
        "    model2 = model2.train()\n",
        "    for x, y in data_loader:\n",
        "        # Forward and Backward propagation\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        out1 = model1(x)\n",
        "        out2 = model2(x)\n",
        "\n",
        "        out2_pred = torch.argmax(out2, 1)\n",
        "        out1_pred = torch.argmax(out1, 1)\n",
        "\n",
        "        if warmups > 0:\n",
        "            model1_loss = criterion(out1, y)\n",
        "            model2_loss = criterion(out2, y)\n",
        "            model1_loss, model2_loss = co_teaching_loss(model1_loss=model1_loss, model2_loss=model2_loss, rt=rt)\n",
        "\n",
        "        elif warmups == 0:\n",
        "            model1_loss = criterion(out1, y)\n",
        "            model2_loss = criterion(out2, y)\n",
        "            model1_loss, model2_loss = co_teaching_loss(model1_loss=model1_loss, model2_loss=model2_loss, rt=rt)\n",
        "            model1_loss_pred = criterion(out1, out2_pred)\n",
        "            model2_loss_pred = criterion(out2, out1_pred)\n",
        "\n",
        "            model1_loss_label = criterion(out1, y)\n",
        "            model2_loss_label = criterion(out2, y)\n",
        "\n",
        "            model1_loss = 1 * model1_loss_pred + .0 * model1_loss_label\n",
        "            model2_loss = 1 * model2_loss_pred + .0 * model2_loss_label\n",
        "            model1_loss, model2_loss = co_teaching_loss(model1_loss=model1_loss, model2_loss=model2_loss, rt=rt)\n",
        "\n",
        "        # loss exchange\n",
        "        optimizer.zero_grad()\n",
        "        model1_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model1.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        model2_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model2.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_loss += (model1_loss.item() + model2_loss.item())\n",
        "\n",
        "        # Compute accuracy\n",
        "        acc = torch.eq(torch.argmax(out1, 1), y).float()\n",
        "        avg_accuracy += acc.mean()\n",
        "        global_step += 1\n",
        "\n",
        "    return avg_accuracy / global_step, avg_loss / global_step, [model1, model2]\n",
        "\n",
        "\n",
        "def test_step(data_loader, gpu: bool, model):\n",
        "    model = model.eval()\n",
        "    global_step = 0\n",
        "    avg_accuracy = 0.\n",
        "    predicted_labels = []\n",
        "\n",
        "    for x, y in data_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        _, preds = torch.max(logits, 1)\n",
        "        acc = torch.eq(torch.argmax(logits, 1), y)\n",
        "        acc = acc.cpu().numpy()\n",
        "        acc = np.mean(acc)\n",
        "        avg_accuracy += acc\n",
        "        global_step += 1\n",
        "        predicted_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "    return avg_accuracy / global_step, predicted_labels\n",
        "\n",
        "\n",
        "def valid_step(data_loader, gpu: bool, model):\n",
        "    model = model.eval()\n",
        "    global_step = 0\n",
        "    avg_accuracy = 0.\n",
        "\n",
        "    for x, y in data_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        logits = model(x)\n",
        "        acc = torch.eq(torch.argmax(logits, 1), y)\n",
        "        acc = acc.cpu().numpy()\n",
        "        acc = np.mean(acc)\n",
        "        avg_accuracy += acc\n",
        "        global_step += 1\n",
        "    return avg_accuracy / global_step\n",
        "\n",
        "\n",
        "def update_reduce_step(cur_step1, num_gradual, tau):\n",
        "    return 1.0 - tau * min(cur_step1 / num_gradual, 1)\n",
        "\n",
        "\n",
        "def train(lr, tau, num_gradual, warmups, gpu = True, epochs = 10):\n",
        "    model1 = CNN(3, 100)\n",
        "    model2 = CNN(3, 100)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "        model1 = nn.DataParallel(model1)\n",
        "        model2 = nn.DataParallel(model2)\n",
        "\n",
        "    model1.to(device)\n",
        "    model2.to(device)\n",
        "\n",
        "    # learning history\n",
        "    train_acc_list = []\n",
        "    test_acc_list = []\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=FLAGS.stop_patience, verbose=False)\n",
        "    criterion = nn.CrossEntropyLoss(reduce=False, label_smoothing=0.2)\n",
        "    optimizer = optim.Adam(chain(model1.parameters(), model2.parameters()), lr, weight_decay=1e-3)\n",
        "    for e in range(epochs):\n",
        "        # update reduce step\n",
        "        curstep1 = e\n",
        "        rt = update_reduce_step(curstep1, num_gradual, tau)\n",
        "\n",
        "        # training step\n",
        "        train_accuracy, avg_loss, model_list = train_step(data_loader=trainLoader,\n",
        "                                                          gpu=gpu,\n",
        "                                                          model_list=[model1, model2],\n",
        "                                                          optimizer=optimizer,\n",
        "                                                          criterion=criterion,\n",
        "                                                          rt=rt,\n",
        "                                                          warmups=warmups)\n",
        "        model1, model2 = model_list\n",
        "\n",
        "        if warmups > 0:\n",
        "            warmups = warmups - 1\n",
        "\n",
        "        # testing/valid step\n",
        "        test_accuracy, predicted_labels = test_step(data_loader=testLoader,\n",
        "                                  gpu=gpu,\n",
        "                                  model=model1)\n",
        "\n",
        "        dev_accuracy = valid_step(data_loader=valLoader,\n",
        "                                  gpu=gpu,\n",
        "                                  model=model1)\n",
        "\n",
        "        train_accuracy = train_accuracy.cpu().data.numpy()\n",
        "        train_acc_list.append(train_accuracy)\n",
        "        test_acc_list.append(test_accuracy)\n",
        "\n",
        "        logging.info(\n",
        "            '{} epoch, Train Loss {}, Train accuracy {}, Dev accuracy {}, Test accuracy {}, Reduce rate {}'.format(e + 1,\n",
        "                                                                                                                avg_loss,\n",
        "                                                                                                                train_accuracy,\n",
        "                                                                                                                dev_accuracy,\n",
        "                                                                                                                test_accuracy,\n",
        "                                                                                                                rt))\n",
        "\n",
        "        # early_stopping(-dev_accuracy, model1, test_acc=test_accuracy)\n",
        "        # if early_stopping.early_stop:\n",
        "        #     logging.info('Training stopped! Best accuracy = {}'.format(max(early_stopping.acc_list)))\n",
        "        #     break\n",
        "\n",
        "    # learning curve plot\n",
        "    return train_acc_list, test_acc_list, epochs, predicted_labels"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set parameters\n",
        "\n",
        "\n",
        "\n",
        "train_acc_list, test_acc_list, epochs, predicted_labels = train(lr=0.001, tau=0.3, warmups=50, num_gradual=30, gpu=True, epochs=30)\n",
        "#Set warmups to 0 to only use prediction co-teaching \n",
        "\n",
        "print(train_acc_list)\n",
        "\n",
        "\n",
        "xrange = [(i + 1) for i in range(epochs)]\n",
        "plt.plot(xrange, train_acc_list, 'b', label='Training accuracy')\n",
        "plt.plot(xrange, test_acc_list, 'r', label='Test accuracy')\n",
        "plt.legend()\n",
        "plt.title('Learning Curve')\n",
        "plt.savefig('l_curve.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xrange = [(i + 1) for i in range(epochs)]\n",
        "plt.plot(xrange, train_acc_list, 'b', label='training accuracy')\n",
        "plt.plot(xrange, test_acc_list, 'r', label='test accuracy')\n",
        "plt.legend()\n",
        "plt.title('Learning curve')\n",
        "plt.savefig('l_curve.png')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyONnvKUvE8P3fcGNKeCKeTR",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
